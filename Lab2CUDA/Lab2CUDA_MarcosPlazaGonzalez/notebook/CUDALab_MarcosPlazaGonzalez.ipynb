{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CUDALab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bx2JSyCHfCq"
      },
      "source": [
        "# Programaciò paral·lela\n",
        "## Lab 2: Color conversion in CUDA\n",
        "\n",
        "Following the previous laboratory based on OpenMP, now we are going to work on the same simple algorithm, in CUDA.\n",
        "\n",
        "The goal of this lab is to learn the basics of CUDA kernels, and CUDA Host code.\n",
        "\n",
        "This new ipython notebook format, will make things easyer since explanation and code will coexist in the same document, and it will be very clear what you need to do.\n",
        "\n",
        "Additionally, having the Colab platform available with NVIDIA GPU's makes it simpler than ever. You can learn CUDA from any system, Mac, Windows, Linux, and any hardware, Intel, AMD, NVIDIA, and possibliy even ARM on tablets. You only need a web browser compatible with Colab.\n",
        "\n",
        "## Structure of the Lab\n",
        "You already know the algorithm from the previous lab, but you may not be familiar with this environment.\n",
        "\n",
        "First we will try to understand a bit this environment, and then we will explain section by section what you have to do. There are 6 sections.\n",
        "\n",
        "You will have to complete code in the 6 sections, and perform experiments and comment the results in a separated report. Use tables and figures that support both the results you collected and the arguments you make to justify the results.\n",
        "\n",
        "## The collab environment for CUDA\n",
        "\n",
        "First of all, you should know that we are executing an iPython notebook in a Google Colab session. The notebook is preconfigured with the type of execution environment we need, a GPU execution environment. But the files we generate, and the pluggins we install or enable, reside on the Google Colab session. All this will be removed when we exit the session either manually or implicitly by closing the browser.\n",
        "\n",
        "In order to have a GPU available when creating a new notebook, you only have to select the execution environment.\n",
        "In Spanish, go to \"Entorno de ejecución->Cambiar tipo de entorno de ejecución\" and then select GPU.\n",
        "\n",
        "But as we already mentioned, this notebook is already configured, so you don't need to do it again.\n",
        "\n",
        "Now, the first thing we will see is that we have the nvcc compiler. We can call many bash commands with ! as the first character, in a code block. Next you will find a code block with a call to nvcc (the nvidia CUDA compiler) with a flag that asks for the compiler version.\n",
        "\n",
        "Click on the block and then a play button will appear on the left. Click on the play button. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNXYi-1xD-Zs",
        "outputId": "10163487-3b39-4880-d585-3912bc075ce8"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4rhjdqSImo2"
      },
      "source": [
        "You can also execute it by placing the cursor inside the code block and pressing Shift+Enter\n",
        "\n",
        "Next you need to install a pluggin, that does not come with the notebook. In the following code block you have the code line to be executed. You will have to execute this code every time you open the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l1NOZW5ET_p",
        "outputId": "7c7bdb10-e5b0-43e3-ac73-23abaacc4c0a"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-yltu7fk1\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-yltu7fk1\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp37-none-any.whl size=4307 sha256=e186dc2633b51457ddb54a4a56b571efdc0ed8663ac2f7ab508936b48d8e9bf1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tzgupgku/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuMz7OOYJVfk"
      },
      "source": [
        "Now, you can compile and execute CUDA code, just by puting the same code you would put ina .cu file, just by adding %%cu as the first line.\n",
        "\n",
        "Next, you have a code example. Try it! Read the comments to help you understand it. It will be very useful for the tasks you have to do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x03T7kcmEgu4",
        "outputId": "94410d3e-3cfc-4990-f78d-9bedc6bbbc04"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// Function to print the cuda errors\n",
        "void cuCheck(cudaError_t err) {\n",
        "    if(err!=cudaSuccess) {\n",
        "          printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "}\n",
        "\n",
        "// This macros help with capturing the possible cuda errors and printing\n",
        "// the error name to help the developer.\n",
        "// Kernels are always asynchronous with respect to the Host, so they don't return\n",
        "// any value. Then to see if any error happened, you should call cudaGetLastError\n",
        "// and pass the result to cuCheck()\n",
        "// Use the macros instead, to make it simpler.\n",
        "#define CU_CHECK(a) cuCheck(a)\n",
        "#define CU_CHECK_LAST_ERROR cuCheck(cudaGetLastError())\n",
        "\n",
        "// Device code or Kernel\n",
        "__global__ void add(int a, int b, int* __restrict d_c) { // restrict doesn't have any effect\n",
        "    *d_c = a * b;\n",
        "}\n",
        "\n",
        "// Host code\n",
        "int main() {\n",
        "    \n",
        "    // Host variables a & b\n",
        "    int a = 3, b = 5, h_c = 0;\n",
        "\n",
        "    // Host variable that will store a Device pointer wich we can later on \n",
        "    // download to the Host.\n",
        "    // As this variable will contain pointers that are only valid in\n",
        "    // the Device (the GPU) it will be invalid to access them from\n",
        "    // Host code. We only can use them in the right cuda API calls\n",
        "    // or inside a cuda Kernel.\n",
        "    // So in this part of the code you won't be able to do d_c[0], for instance\n",
        "    int *d_c;\n",
        "\n",
        "    // Size of the data contained in variables a, b and c.\n",
        "    int dataSize = sizeof(int);\n",
        "\n",
        "    // Reserve Device memory using the cuda API\n",
        "    // cudaMalloc will place a Device pointer inside d_c.\n",
        "    // Calling a cuda API function always returns a error code\n",
        "    CU_CHECK(\n",
        "        cudaMalloc((void **)&d_c, dataSize) \n",
        "    );\n",
        "\n",
        "    // Launch add() kernel on GPU\n",
        "    // Notice that a and b are not pointers. Therefore the kernel call will\n",
        "    // copy their values but the variables inside the kernel will not be the same.\n",
        "    // If we modify a and b inside the kernel, it will not change a and b in this\n",
        "    // Host code. This, indeed is the same behavior as any C/C++ function call.\n",
        "    // In the case of d_c, it will copy the pointer contained in d_c, \n",
        "    // so we will be able to modify the contents of d_c from the kernel. But to read \n",
        "    // them from this Host code, we will have to do something else.\n",
        "    add<<<1,1>>>(a, b, d_c); // only values are passed\n",
        "    CU_CHECK_LAST_ERROR;\n",
        "\n",
        "    CU_CHECK(\n",
        "        // Copy result back to host\n",
        "        cudaMemcpy(&h_c, d_c, dataSize, cudaMemcpyDeviceToHost)\n",
        "    );\n",
        "\n",
        "    printf(\"Result of multiplying %d * %d is %d\\n\\n\",a,b,h_c);\n",
        "\n",
        "    int numDevs=0;\n",
        "    CU_CHECK(\n",
        "        cudaGetDeviceCount(&numDevs)\n",
        "    );\n",
        "\n",
        "    cudaDeviceProp prop;\n",
        "    CU_CHECK(\n",
        "        cudaGetDeviceProperties(&prop, 0)\n",
        "    );\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "          prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "          prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "          2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    printf(\"Num devices %d\\n\", numDevs);\n",
        "    // Cleanup\n",
        "    CU_CHECK(\n",
        "      cudaFree(d_c)\n",
        "    );\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result of multiplying 3 * 5 is 15\n",
            "\n",
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064000\n",
            "\n",
            "Num devices 1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajyZJvBCZR7p"
      },
      "source": [
        "Ok, cool! But what if I want to have some code in a .h file, the cuda kernels in an other .h file, and include both so that I can reuse code?\n",
        "\n",
        "Ok, let's try to put the macros and cuCheck function in a .h file, the kernel in an other .h file and the rest in a .cu file, and compile and execute everything. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "SlObGBaEZ5M7",
        "outputId": "7ee00536-eec8-466a-ab1e-5fc7f8ab4e4a"
      },
      "source": [
        "%%cuda --name utils.h\n",
        "#include <iostream>\n",
        "\n",
        "void cuCheck(cudaError_t err, const std::string message = \"CUDA error:\") {\n",
        "  if(err!=cudaSuccess) {\n",
        "    std::cout << message << \" ERROR \" << cudaGetErrorString(err) << std::endl;\n",
        "  }\n",
        "}\n",
        "#define CU_CHECK(a) cuCheck(a)\n",
        "#define CU_CHECK2(a, b) cuCheck(a, b)\n",
        "#define CU_CHECK_LAST_ERROR cuCheck(cudaGetLastError())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/utils.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "DEoYz9uug7om",
        "outputId": "a1bacee6-74a6-4e78-9327-182dc74ef01d"
      },
      "source": [
        "%%cuda --name kernels.h\n",
        "__global__ void add(int a, int b, int* __restrict d_c) {\n",
        "    *d_c = a * b;\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/kernels.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdHjgZRHlSGe",
        "outputId": "9ef01024-611b-4ada-c7da-49437f8c86e7"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <fstream>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/kernels.h\"\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "// Host code\n",
        "int main() {\n",
        "    int a = 3, b = 5, h_c = 0;\n",
        "    int *d_c;\n",
        "    int dataSize = sizeof(int);\n",
        "\n",
        "    CU_CHECK(cudaMalloc((void **)&d_c, dataSize));\n",
        "\n",
        "    add<<<1,1>>>(a, b, d_c);\n",
        "\n",
        "    CU_CHECK_LAST_ERROR;\n",
        "\n",
        "    CU_CHECK(cudaMemcpy(&h_c, d_c, dataSize, cudaMemcpyDeviceToHost));\n",
        "\n",
        "    int numDevs=0;\n",
        "\n",
        "    CU_CHECK(cudaGetDeviceCount(&numDevs));\n",
        "\n",
        "    ofstream gpu_info(\"/content/src/gpu_information.txt\"); // store the gpu info into a file will be useful. This file may be downloaded lately\n",
        "\n",
        "    cudaDeviceProp prop;\n",
        "    CU_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
        "\n",
        "\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    gpu_info << \"Device Number: \" << 0 << \"\\n\";\n",
        "\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    gpu_info << \"Device name: \" << prop.name << \"\\n\";\n",
        "\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\", prop.memoryClockRate);\n",
        "    gpu_info << \"Memory Clock Rate (KHz): \" << prop.memoryClockRate << \"\\n\";\n",
        "\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\", prop.memoryBusWidth);\n",
        "    gpu_info << \"Memory Bus Width (bits): \" << prop.memoryBusWidth << \"\\n\";\n",
        "\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\", 2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    gpu_info << \"Peak Memory Bandwidth (GB/s): \" << 2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6 <<\"\\n\";\n",
        "\n",
        "    printf(\"Num devices %d\\n\", numDevs);\n",
        "    gpu_info << \"Num devices \" << numDevs << \"\\n\";\n",
        "\n",
        "    printf(\"Result of multiplying %d * %d is %d\\n\",a,b,h_c);\n",
        "    gpu_info << \"Result of multiplying \" << a << \"*\" << b << \" is \" << h_c << \"\\n\";\n",
        "\n",
        "\n",
        "    gpu_info.close();\n",
        "\n",
        "    CU_CHECK(cudaFree(d_c)); \n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064000\n",
            "\n",
            "Num devices 1\n",
            "Result of multiplying 3 * 5 is 15\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8rWgfJ8hN3m"
      },
      "source": [
        "VERY IMPORTANT!!! On each Colab session, the GPU that Google Colab provides can be different. Take it into account when you perform experiments, so that you compare results for the same GPU.\n",
        "\n",
        "If you have to repeat all the experiments, well, it's not that hard, just click play in all the code blocks one by one.\n",
        "\n",
        "Great!! Now we can start the lab :-D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX0Yv50Wh4nY"
      },
      "source": [
        "##Section 1:\n",
        "\n",
        "Try to complete the following code, and make it compile. Remember that you have some slides and documents, and the CUDA API specification in the following link: https://docs.nvidia.com/cuda/cuda-runtime-api/index.html\n",
        "\n",
        "Also, you can search in Google, things like \"How to allocate CUDA memory\". And so on. Be brave! Is not so difficult.\n",
        "\n",
        "### First, complete the allocation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "u6Bj54YdVsjI",
        "outputId": "28a6c041-9d3a-4b77-e407-3fb4fc73d340"
      },
      "source": [
        "%%cuda --name memory_functions.h\n",
        "void allocGPUData(int width, int height, uchar3** d_brg, uchar4** d_rgba){\n",
        "  // Alloc gpu pointers\n",
        "  CU_CHECK2(cudaMalloc(d_brg, sizeof(uchar3)*width*height), \"Alloc d_brg:\");\n",
        "  // Can you finish this one? Replace cudaSucces with the proper cuda API call\n",
        "  CU_CHECK2(cudaMalloc(d_rgba, sizeof(uchar4)*width*height), \"Alloc d_rgba:\");\n",
        "}\n",
        "\n",
        "void copyAndInitializeGPUData(int width, int height, uchar3* h_brg, uchar3* d_brg, uchar4* d_rgba, cudaStream_t stream=0) {\n",
        "  // Copy data to GPU\n",
        "  CU_CHECK2(cudaMemcpy(d_brg, h_brg, width*height*sizeof(uchar3), cudaMemcpyHostToDevice), \"Copy h_brg to d_brg:\");\n",
        "  // Init output buffer to 0\n",
        "  CU_CHECK2(cudaMemset(d_rgba, 0, width*height*sizeof(uchar4)), \"Memset d_rgba:\");\n",
        "}\n",
        "\n",
        "void freeCUDAPointers(uchar3* d_brg, uchar4* d_rgba) {\n",
        "  // Free cuda pointers. Replace the cudaErrorInvalidValue flag\n",
        "  // with the proper cuda API call, to free the GPU pointers\n",
        "  CU_CHECK2(cudaFree(d_brg), \"Cuda free d_bgr:\");\n",
        "  CU_CHECK2(cudaFree(d_rgba), \"Cuda free d_rgba:\");\n",
        "  // Clean GPU device\n",
        "  CU_CHECK2(cudaDeviceReset(), \"Cuda device reset:\");\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/memory_functions.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s30mTBEIWcX6"
      },
      "source": [
        "### When completed, test that they work with this small main function. If you execute it without completing the previous code, it will show some errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tup9fxlwWl64",
        "outputId": "bcdebb90-7a24-4013-e7f2-bd0e5d10029a"
      },
      "source": [
        "%%cu\n",
        "#include <cuda.h>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "\n",
        "#define WIDTH 10\n",
        "#define HEIGHT 10\n",
        "\n",
        "int main() {\n",
        "\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba);\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSfdiwiHZKPT"
      },
      "source": [
        "### Ok, now that we have the allocation, copy and free functions implemented, let's continue with the CPU function that will check the results. This one it's already implemented, you only need to click play to have it available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "2fywcbihaBaR",
        "outputId": "a0c13f4d-c961-40e0-a5b9-79a1f87ac33b"
      },
      "source": [
        "%%cuda --name check_results.h\n",
        "bool checkResults(uchar4* rgba, uchar3* brg, int size) {\n",
        "  bool correct = true;\n",
        "  for (int i=0; i < size; ++i) {\n",
        "    // In case you want to see actual values\n",
        "    if (i==3) {\n",
        "      unsigned char x, y, z, w;\n",
        "      x = rgba[i].x;\n",
        "      y = rgba[i].y;\n",
        "      z = rgba[i].z;\n",
        "      w = rgba[i].w;\n",
        "      std::cout << \"First position x=\" << (unsigned int)x << \" y=\" << (unsigned int)y << \" z=\" << (unsigned int)z << \" w=\" << (unsigned int)w << std::endl;\n",
        "    }\n",
        "    correct &= rgba[i].x == brg[i].y;\n",
        "    correct &= rgba[i].y == brg[i].z;\n",
        "    correct &= rgba[i].z == brg[i].x;\n",
        "    correct &= rgba[i].w == 255;\n",
        "  }\n",
        "  return correct;\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/check_results.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl-YagCWaZOS"
      },
      "source": [
        "### Now the interesting part, the kernel and the code to configure and launch it. The kernel it's almost exactly the same code as the OpenMP lab, only we replaced the forloops with something that you need to implement.\n",
        "\n",
        "Remember, that we have threads with indexes. This indexes are used to tell each CUDA thread, which data do they have to read or write.\n",
        "\n",
        "The structs that contain those indexes are in the documentation you have available in campusvirtual. Please check the docs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AH5pXNc6L4_"
      },
      "source": [
        "Check Hardware Constraints in https://stackoverflow.com/questions/9985912/how-do-i-choose-grid-and-block-dimensions-for-cuda-kernels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Et3Bww3PbMgw",
        "outputId": "a734a1f7-b686-497e-b179-3b2271548bec"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "// BIDIMENSIONAL KERNEL\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  // Identificar cada thread de forma unica\n",
        "  int x = threadIdx.x + (blockIdx.x * blockDim.x); //Use the thread id and block id's to compute x \n",
        "  int y = threadIdx.y + (blockIdx.y * blockDim.y); //Use the thread id and block id's to compute y\n",
        "\n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width && y < height) {\t\n",
        "\t    rgba[width * y + x].x = brg[width * y + x].y;\n",
        "\t    rgba[width * y + x].y = brg[width * y + x].z;\n",
        "\t    rgba[width * y + x].z = brg[width * y + x].x;\n",
        "\t    rgba[width * y + x].w = 255;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(256, 4, 1);\n",
        "  dim3 grid(ceil(width/(float)block.x),ceil(height/(float)block.y) , 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "\n",
        "  CU_CHECK2(cudaFree(d_secondData), \"cudaFree in executeKernelconvertBRG2RGBA\");\n",
        "}"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt2Ty-GmcSY_"
      },
      "source": [
        "### MAIN EXPERIMENT \n",
        "Try all the previous code, with the following main. If you did not finish all the previous code, this file will show some execution errors.\n",
        "\n",
        "The code is divided in two parts, one to define the parameters of the experiment and the other one is the main function with the experiment it self.\n",
        "\n",
        "The experiment is the code that creates a BRG image in CPU, allocates GPU memory, copies the BRG image to GPU memory, and executes a GPU kernel to convert the BRG image into a RGBA image. The output of the kernel is another GPU pointer, so after the kernel execution, we have to copy back the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "FwcU52z8B--N",
        "outputId": "d0edc661-3683-4faf-8bfc-4b4491d8d288"
      },
      "source": [
        "%%cuda --name experiment_settings.h\n",
        "#pragma once\n",
        "#define WIDTH 3840\n",
        "#define HEIGHT 2160\n",
        "#define EXPERIMENT_ITERATIONS 100"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/experiment_settings.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "ZT9ep3h_ijOO",
        "outputId": "8c6c94b1-9d15-44e6-8e84-a3b08c6bfee8"
      },
      "source": [
        "%%cuda --name experiment.h\n",
        "#include <cuda.h>\n",
        "#include <chrono>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "#include \"/content/src/check_results.h\"\n",
        "#include \"/content/src/cuda_launcher.h\"\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "void executeExperiment() {\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  int bar_widht = HEIGHT/3;\n",
        "\n",
        "  // Alloc and generate BRG bars.\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  for (int i=0; i < WIDTH * HEIGHT; ++i) {\n",
        "    if (i < bar_widht) {\n",
        "      uchar3 temp = {255, 0, 0};\n",
        "      h_brg[i] = temp; \n",
        "    } else if (i < bar_widht*2) {\n",
        "      uchar3 temp = {0, 255, 0};\n",
        "      h_brg[i] = temp;\n",
        "    } else { \n",
        "      uchar3 temp = {0, 0, 255};\n",
        "      h_brg[i] = temp;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Alloc RGBA pointers\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  // Alloc gpu pointers\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "  \n",
        "  // Prepare and copy data to GPU\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba);\n",
        "\n",
        "  // Execute the GPU kernel\n",
        "  executeKernelconvertBRG2RGBA(WIDTH, HEIGHT, d_brg, d_rgba, EXPERIMENT_ITERATIONS);\n",
        "\n",
        "  // Copy data back from GPU to CPU, without streams\n",
        "  CU_CHECK2(cudaMemcpy(h_rgba, d_rgba, sizeof(uchar4)*WIDTH*HEIGHT, cudaMemcpyDeviceToHost), \"Cuda memcpy Device to Host: \");\n",
        "    \n",
        "  // Check results\n",
        "  bool ok = checkResults(h_rgba, h_brg, WIDTH*HEIGHT);\n",
        "  if (ok) {\n",
        "      std::cout << \"Executed!! Results OK.\" << std::endl;\n",
        "  } else {\n",
        "      std::cout << \"Executed!! Results NOT OK.\" << std::endl;\n",
        "  }\n",
        "\n",
        "  // Free CPU pointers\n",
        "  free(h_rgba);\n",
        "  free(h_brg);\n",
        "\n",
        "  // Free cuda pointers\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "}"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/experiment.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5RA9zDI7Z0V",
        "outputId": "ffb3f36c-fb98-4ab2-ffd5-2554f83df2bc"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertBRG2RGBA time for 100 iterations = 48673us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GqpbCcVdK83"
      },
      "source": [
        "##Section 2:\n",
        "Implement a version of the kernel and launcher that uses a one dimensional cuda GRID. That is, there is no more x and y, only x.\n",
        "\n",
        "Modify the code below, click play, and then click play in the Main Experiment block, in Section 1.\n",
        "\n",
        "Try different values of BLOCK_SIZE.\n",
        "\n",
        "Check if there is any execution time improvement, compared to Section 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "gLPY7trNd1SG",
        "outputId": "849f783c-064b-4717-9f0e-64fd3018d288"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "\n",
        "#define BLOCK_SIZE 128\n",
        "// blockDim.x * blockDim.y * blockDim.z <= 1,024\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = threadIdx.x + (blockIdx.x * blockDim.x); //Use the thread id and block id's to compute x \n",
        "  \n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width * height) {\t\n",
        "\t    rgba[x].x = brg[x].y;\n",
        "\t    rgba[x].y = brg[x].z;\n",
        "\t    rgba[x].z = brg[x].x;\n",
        "\t    rgba[x].w = 255;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozsLq2rt__xl",
        "outputId": "4ff46711-08a5-491f-edb2-ddb1e104162b"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertBRG2RGBA time for 100 iterations = 39355us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExMCHf_RD3Wd"
      },
      "source": [
        "Change the experiment settings, by executing more iterations and compare the unidimensional kernel with the bidimensional kernel.\n",
        "\n",
        "Comment the results in the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "02hd2qlUEOsg",
        "outputId": "7f3c28eb-df96-4f70-849c-c5403067cb76"
      },
      "source": [
        "%%cuda --name experiment_settings.h\n",
        "#pragma once\n",
        "#define WIDTH 3840\n",
        "#define HEIGHT 2160\n",
        "#define EXPERIMENT_ITERATIONS 100 //try different values"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/experiment_settings.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3gBy530ektk"
      },
      "source": [
        "## Section 3:\n",
        "\n",
        "Starting from Section 2, (use the best BLOCK_SIZE you found) try to optimize the memory accesses in some way, without using shared memory.\n",
        "\n",
        "Comment in the report which memory access problems you observe. Are the memory accesses aligned, and therfore coalesced?\n",
        "\n",
        "Remember that opposite to what the CPU compilers do, the nvcc compiler does not optimize the memory accesses in structs\n",
        "\n",
        "Remember also that GPU memory is organized in blocks of 4 bytes, and any array based on data elements that are not multiple of 2, will not be alligned. To be coalesced (specially in old architectures), it also has to be multiple of 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "RmP4PDZ-e4lG",
        "outputId": "c7163290-fc59-4cb8-f79c-aba454816cec"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "\n",
        "#define BLOCK_SIZE 128\n",
        "// 128 va mejor\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL BETTER MEMORY ACCESS\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = threadIdx.x + (blockIdx.x * blockDim.x); //Use the same code as in section 2 in this line\n",
        "  \n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width * height) {\n",
        "      uchar3 tmp = brg[x]; // we copy the whole uchar3 to private memory \n",
        "      rgba[x] = make_uchar4(tmp.y, tmp.z, tmp.x, 255); // write all the values at the same time\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMDGsQjPAkIv",
        "outputId": "7b4c6d34-1479-4b2e-e801-fb088534d98a"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertBRG2RGBA time for 100 iterations = 29154us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPECp0T6IU0Z"
      },
      "source": [
        "##Section 4:\n",
        "Now optimize the GPU memory accesses so that each thread always reads at least one element of 4 bytes. Use shared memory to that end.\n",
        "\n",
        "Look at the PDF Lab2CUDA in the campus, an read the last two pages. There you have a graphical explanation of the kernel issues. For this section you only need to understand the first figure.\n",
        "\n",
        "About shared memory: we will refresh some concepts.\n",
        "\n",
        "Shared memory, is a kind of memory that is visible only by the cuda threads of a thread block. Cuda threads from different thread blocks can not see the shared memory of other threadblocks.\n",
        "\n",
        "Shared memory is a limited resource. Depending on the GPU model, you may have from 32KB to 64KB of shared memory. Additionally, this memory is not used only by one threadblock. It is partitioned in as many independent blocks as thread blocks can execute in a single Streaming Multiprocessor (check the documentation if you don't know what a SM is). \n",
        "\n",
        "So when you are defining the amount of shared memory you want, you are defining the amount of memory, every thread block will have available.\n",
        "\n",
        "If you reserve 64KB of shared memory, in a GPU that has this capacity, only one thread block will execute on each SM, which is super slow. Each SM can concurrently execute from 8 to 32 thread blocks. For the best performance, you usually want the greatest amount of thread blocks active on each SM.\n",
        "\n",
        "Therefore, you what to use the least shared memory possible, and only use it when it has clear benefits.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "MKRs22QeIvwj",
        "outputId": "0159a127-277e-4590-d25a-3201723a7f36"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "// Try different vaues of BLOCK_SIZE\n",
        "#define BLOCK_SIZE 128 //256\n",
        "// Number of 4 byte elements that we can make out of BLOCK_SIZE elements of 3 bytes\n",
        "#define N_ELEMS_3_4_TBLOCK (BLOCK_SIZE * 3)/4\n",
        "#define N_ELEMS_3_4_IMAGE (WIDTH*HEIGHT * 3)/4\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL SHARED MEMORY\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int position = threadIdx.x + (blockIdx.x * N_ELEMS_3_4_TBLOCK); // use N_ELEMS_3_4_TBLOCK to compute the position of each thread when we read brg as if it had elements of 4 bytes\n",
        "  __shared__ uchar4 bgrShared[N_ELEMS_3_4_TBLOCK]; // using shared memory here\n",
        "  \n",
        "  if(threadIdx.x < N_ELEMS_3_4_TBLOCK && position < N_ELEMS_3_4_IMAGE) {\n",
        "      uchar4* temp = reinterpret_cast<uchar4*>(brg);\n",
        "      bgrShared[threadIdx.x] = temp[position];\n",
        "  }\n",
        "\n",
        "  __syncthreads(); // Why do we need to syncronize all the threads? Para esperar a que la trasferencia de device to shared se haga efectiva\n",
        "  \n",
        "  position = threadIdx.x + (blockIdx.x * blockDim.x); // recompute position without N_ELEMS_3_4_TBLOCK to write the results\n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (position < width*height) {\t\n",
        "        uchar3 local = reinterpret_cast<uchar3*>(bgrShared)[threadIdx.x];\n",
        "        rgba[position] = make_uchar4(local.y,local.z,local.x,255);\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4E_ErYqLAtVU",
        "outputId": "30e587fc-897f-40bc-a673-9abbc70ff10c"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertBRG2RGBA time for 100 iterations = 32332us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGPcLMKz6a0d"
      },
      "source": [
        "##Section 5:\n",
        "\n",
        "Now, following the explanation in the pdf document provided along with this colab, try to implement the described algorithm. Take into account that the piece of code that reads from temp variables and writes in pix_write, requires some changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DvT4yBO6ck2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "fa001a9a-7977-4d7c-9091-4215a6f67760"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "// Try different vaues of BLOCK_SIZE\n",
        "#define BLOCK_SIZE 128\n",
        "\n",
        "// Number of 4 byte elements that we can make out of BLOCK_SIZE elements of 3 bytes\n",
        "#define N_ELEMS_3_4_TBLOCK (BLOCK_SIZE * 3)/4\n",
        "#define N_ELEMS_3_4_IMAGE (WIDTH*HEIGHT * 3)/4\n",
        "#define N_RW_THREADS N_ELEMS_3_4_TBLOCK/3\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL SHARED MEMORY\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "    \n",
        "  /* Iniciamos la transferencia de la imagen brg de device a shared \n",
        "  (Usamos 96 hilos que son los hilos necesarios (3/4 del total) para traer la informacion \n",
        "  de 128 pixeles de un tamaño de 3 bytes en bloques de 4 bytes) */\n",
        "\n",
        "  int position = threadIdx.x + (blockIdx.x * N_ELEMS_3_4_TBLOCK); // use N_ELEMS_3_4_TBLOCK to compute the position of each thread when we read brg as if it had elements of 4 bytes\n",
        "  __shared__ uchar4 bgrShared[N_ELEMS_3_4_TBLOCK]; // using shared memory here to hold the 128 pixels of 3 bytes each (taking into account memory structure)\n",
        "  \n",
        "  if(threadIdx.x < N_ELEMS_3_4_TBLOCK && position < N_ELEMS_3_4_IMAGE) { // avoid segmentation fault\n",
        "      uchar4* temp = reinterpret_cast<uchar4*>(brg);\n",
        "      bgrShared[threadIdx.x] = temp[position]; \n",
        "  }\n",
        "\n",
        "  __syncthreads(); // Esperamos a que la transferencia se haga efectiva\n",
        "    \n",
        "  __shared__ uchar4 pix_write[BLOCK_SIZE]; // Queremos escribir 128 pixeles (cada uno de 4 bytes)\n",
        "    \n",
        "\t// Protection to avoid segmentation fault (divergence)\n",
        "\tif (threadIdx.x < N_RW_THREADS) {\t // Usamos 1/4 del total de hilos necesarios para copiar la info de 4 pixeles en variables temporales\n",
        "        /* Lectura de shared memory */\n",
        "        uchar4 temp1, temp2, temp3;\n",
        "        temp1 = bgrShared[3 * threadIdx.x];\n",
        "        temp2 = bgrShared[3 * threadIdx.x + 1];\n",
        "        temp3 = bgrShared[3 * threadIdx.x + 2];\n",
        "                                     \n",
        "        /* Escritura en memoria compartida */\n",
        "        pix_write[4 * threadIdx.x] = make_uchar4(temp1.y, temp1.z, temp1.x, 255);\n",
        "        pix_write[4 * threadIdx.x + 1] = make_uchar4(temp2.x, temp2.y, temp1.w, 255);\n",
        "        pix_write[4 * threadIdx.x + 2] = make_uchar4(temp2.w, temp3.x, temp2.z, 255);\n",
        "        pix_write[4 * threadIdx.x + 3] = make_uchar4(temp3.z, temp3.w, temp3.y, 255);\n",
        "\n",
        "\t}\n",
        "\n",
        "  __syncthreads();\n",
        "  \n",
        "  position = threadIdx.x + (blockIdx.x * blockDim.x);\n",
        "  rgba[position] = pix_write[threadIdx.x];\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_brg, d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyyqgRAXJ9it",
        "outputId": "7a3b9939-a530-43e8-da3d-a36886fdb806"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertBRG2RGBA time for 100 iterations = 36089us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MazFSZvFO9lT"
      },
      "source": [
        "##Section 6:\n",
        "\n",
        "Change all the host code necessary, to use cuda streams. Here you have an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EvgoMT5PVYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4bd6b47-22c6-4b5e-f4f3-5ce179be1c36"
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <chrono>\n",
        "\n",
        "__global__ void square(int* d_input, int* d_output) {\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    int val = d_input[x];\n",
        "    // We exploit the temporal locality of the value stored in d_output[x]\n",
        "    d_output[x] = val*val;\n",
        "}\n",
        "\n",
        "static const size_t dataSize = sizeof(int)*1024;\n",
        "using namespace std;\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    int *h_input, *h_output;\n",
        "    h_input = (int*)malloc(dataSize);\n",
        "    h_output = (int*)malloc(dataSize);\n",
        "\n",
        "    for (int i=0; i<1024; ++i) h_input[i]=i;\n",
        "\n",
        "    int *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, dataSize);\n",
        "    cudaMalloc(&d_output, dataSize);\n",
        "\n",
        "    // Creation of cuda stream\n",
        "    cudaStream_t stream;\n",
        "    cudaStreamCreate(&stream);\n",
        "\n",
        "    dim3 block(512);\n",
        "    dim3 grid(2);\n",
        "\n",
        "    // Start measuring time here\n",
        "    auto t1 = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    // The CPU thread does not wait that any of the following actions finish\n",
        "    // It only asks the GPU to do the copies and the kernel and continues\n",
        "    cudaMemcpyAsync(d_input, h_input, dataSize, cudaMemcpyHostToDevice, stream);\n",
        "    square<<<grid, block, 0, stream>>>(d_input, d_output);\n",
        "    cudaMemcpyAsync(h_output, d_output, dataSize, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    auto t2 = std::chrono::high_resolution_clock::now();\n",
        "    auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "    std::cout << \"Time elapsed for CPU execution is \"<< duration << \"us\" << std::endl;\n",
        "\n",
        "    // Here, we wait for all the orders enqueued in stream, to finish.\n",
        "    cudaStreamSynchronize(stream);\n",
        "\n",
        "    bool correct = true;\n",
        "    for (int i=0; i<1024; ++i) correct &= h_output[i] == i*i;\n",
        "\n",
        "    std::cout << \"Finished and results are \" << (correct ? \"correct.\" : \"not correct.\") << std::endl;\n",
        "\n",
        "    cudaStreamDestroy(stream);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed for CPU execution is 51us\n",
            "Finished and results are correct.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4VK6ylPWaEY"
      },
      "source": [
        "Modify this code, to use streams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAn_ggnrbAWq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "72335e3b-ce9e-413c-a2d4-e374e682d0f5"
      },
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "\n",
        "#define BLOCK_SIZE 128\n",
        "// 128 va mejor\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL BETTER MEMORY ACCESS\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height, cudaStream_t stream=0) {\n",
        "  int x = threadIdx.x + (blockIdx.x * blockDim.x); //Use the same code as in section 2 in this line\n",
        "  \n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width * height) {\n",
        "      uchar3 tmp = brg[x]; // we copy the whole uchar3 to private memory \n",
        "      rgba[x] = make_uchar4(tmp.y, tmp.z, tmp.x, 255); // write all the values at the same time\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yz3kVFkrWOlB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "e8e23deb-60dc-4b0f-d4aa-089888efd922"
      },
      "source": [
        "%%cuda --name memory_functions.h\n",
        "void allocGPUData(int width, int height, uchar3** d_brg, uchar4** d_rgba){\n",
        "  \n",
        "  // Alloc gpu pointers\n",
        "  CU_CHECK2(cudaMalloc(d_brg, sizeof(uchar3)*width*height), \"Alloc d_brg:\");\n",
        "  // Can you finish this one? Replace cudaSucces with the proper cuda API call\n",
        "  CU_CHECK2(cudaMalloc(d_rgba, sizeof(uchar4)*width*height), \"Alloc d_rgba:\");\n",
        "}\n",
        "void copyAndInitializeGPUData(int width, int height, uchar3* h_brg, uchar3* d_brg, uchar4* d_rgba, cudaStream_t stream=0) {\n",
        "  // Copy data to GPU\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_brg, h_brg, width*height*sizeof(uchar3), cudaMemcpyHostToDevice, stream), \"Copy h_brg to d_brg:\");\n",
        "  // Init output buffer to 0\n",
        "  CU_CHECK2(cudaMemsetAsync(d_rgba, 0, width*height*sizeof(uchar4), stream), \"Memset d_rgba:\");\n",
        "}\n",
        "void freeCUDAPointers(uchar3* d_brg, uchar4* d_rgba) {\n",
        "  // Free cuda pointers. Replace the cudaErrorInvalidValue flag\n",
        "  // with the proper cuda API call, to free the GPU pointers\n",
        "  CU_CHECK2(cudaFree(d_brg), \"Cuda free d_bgr:\");\n",
        "  CU_CHECK2(cudaFree(d_rgba), \"Cuda free d_rgba:\");\n",
        "  // Clean GPU device\n",
        "  CU_CHECK2(cudaDeviceReset(), \"Cuda device reset:\");\n",
        "}"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/memory_functions.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymPFWGqkWew-"
      },
      "source": [
        "And also modify this code, so that mem copies from CPU to GPU and from GPU to CPU use an stream, and are not blocking.\n",
        "\n",
        "Additionally, add a chrono between the first memcpy (included) and the cudaStreamSynchronize. This is the time you will have to compare.\n",
        "\n",
        "Follow the indications in the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih1ADZpjWeFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "97e76c05-50bf-418f-b357-e5f154cd6aa9"
      },
      "source": [
        "%%cuda --name experiment.h\n",
        "#include <cuda.h>\n",
        "#include <chrono>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "#include \"/content/src/check_results.h\"\n",
        "#include \"/content/src/cuda_launcher.h\"\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "void executeExperiment() {\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  int bar_widht = HEIGHT/3;\n",
        "\n",
        "  // Alloc and generate BRG bars.\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  for (int i=0; i < WIDTH * HEIGHT; ++i) {\n",
        "    if (i < bar_widht) {\n",
        "      uchar3 temp = {255, 0, 0};\n",
        "      h_brg[i] = temp; \n",
        "    } else if (i < bar_widht*2) {\n",
        "      uchar3 temp = {0, 255, 0};\n",
        "      h_brg[i] = temp;\n",
        "    } else { \n",
        "      uchar3 temp = {0, 0, 255};\n",
        "      h_brg[i] = temp;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Alloc RGBA pointers\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  // Alloc gpu pointers\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "\n",
        "  // Declare and create the cuda stream\n",
        "  cudaStream_t stream;\n",
        "  cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking); // Creamos el stream con el flag \"no bloqueante\"\n",
        "  \n",
        "  // Start measuring time here\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba, stream);\n",
        "\n",
        "  // Execute the GPU kernel\n",
        "  executeKernelconvertBRG2RGBA(WIDTH, HEIGHT, d_brg, d_rgba, EXPERIMENT_ITERATIONS, stream); // usar 1 en lugar de EXPERIMENT_ITERATIONS\n",
        "\n",
        "  // Copy data back from GPU to CPU\n",
        "  CU_CHECK2(cudaMemcpyAsync(h_rgba, d_rgba, sizeof(uchar4)*WIDTH*HEIGHT, cudaMemcpyDeviceToHost, stream), \"Cuda memcpy Device to Host: \");\n",
        "\n",
        "  // Synchronize the stream here\n",
        "  cudaStreamSynchronize(stream);\n",
        "\n",
        "  // Stop measuring time here, and print it\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"Time elapsed for CPU execution is \"<< duration << \"us\" << std::endl;\n",
        "    \n",
        "  // Check results\n",
        "  bool ok = checkResults(h_rgba, h_brg, WIDTH*HEIGHT);\n",
        "  if (ok) {\n",
        "      std::cout << \"Executed!! Results OK.\" << std::endl;\n",
        "  } else {\n",
        "      std::cout << \"Executed!! Results NOT OK.\" << std::endl;\n",
        "  }\n",
        "\n",
        "  // Destroy cuda stream\n",
        "  cudaStreamDestroy(stream);\n",
        "  \n",
        "  // Free CPU pointers\n",
        "  free(h_rgba);\n",
        "  free(h_brg);\n",
        "\n",
        "  // Free cuda pointers\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "}"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'File written in /content/src/experiment.h'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cb3WvwUW6E7"
      },
      "source": [
        "Do the following:\n",
        "\n",
        "1.   Use the fastest kernel version.\n",
        "2.   Use number of iterations = 1.\n",
        "3.   Compare the same kernel, with the original Host code, and this new Host code.\n",
        "4.   To do so, you can use the code you do now, you only need to set stream=0 in order to simulate the original code.\n",
        "5.   Execute it with the following code.\n",
        "6.   Compare and try to explain the performance difference in the report.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icJq5yuCW9f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3730e110-e9b6-4c6a-c096-f4171925d88c"
      },
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "convertBRG2RGBA time for 100 iterations = 29141us\n",
            "Time elapsed for CPU execution is 54806us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YN9ouBn_TlFQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}